\documentclass[10pt,twoside,a4paper]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
%\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\usepackage{iflang}

\usepackage[ngerman]{babel}
%\usepackage[english]{babel}

\IfLanguageName{ngerman}{
\newcommand*{\WinterSem}{Wintersemester}
\newcommand*{\SummerSem}{Sommersemester}
}{
\newcommand*{\WinterSem}{winter term}
\newcommand*{\SummerSem}{summer term}
}

% Calculate semester based on \month and \year
\ifnum\month<4
\newcounter{LastYear}
\setcounter{LastYear}{\year}
\addtocounter{LastYear}{-1}
\newcommand{\SemesterShort}{WS \theLastYear/\the\year}
\newcommand{\SemesterLong}{\WinterSem~\theLastYear/\the\year}
\else\ifnum\month>9
\newcounter{NextYear}
\setcounter{NextYear}{\year}
\addtocounter{NextYear}{1}
\newcommand{\SemesterShort}{WS \the\year/\theNextYear}
\newcommand{\SemesterLong}{\WinterSem~\the\year/\theNextYear}
\else
\newcommand{\SemesterShort}{SS \the\year}	
\newcommand{\SemesterLong}{\SummerSem~\the\year}
\fi
\fi

\begin{document}

\author{
  \IEEEauthorblockN{David Roppelt\\}
  \IEEEauthorblockA{Email: david.roppelt@fau.de}
}

\title{Textkompressionsalgorithmen in Datenbanken - Eine Forschungsfeldübersicht
	%brauch ich den IEEEmemebership command?
\IfLanguageName{ngerman}{
  \thanks{Dieser Beitrag entstand im Rahmen des ``Big Data Seminar''s, das im
  \SemesterLong~vom Lehrstuhl für Informatik 6 (Datenmanagement) der
  Friedrich-Alexander Universität Erlangen-Nürnberg durchgef"uhrt wurde.}
}{
  \thanks{This paper was written as part of the ``Big Data Seminar''
  which was organized by the Chair of Computer Science 6 (Data Management) at the
  Friedrich-Alexander Universität Erlangen-Nürnberg during \SemesterLong.}
}}




% The paper headers
\markboth{Big Data Seminar \SemesterShort}{David Roppelt: Textkompressionsalgorithmen in Datenbanken - Eine Forschungsfeldübersicht}

\maketitle

\begin{abstract}
	Leider wird diese Arbeit vom Prokrastination Final Boss geschrieben, welcher dann an seinen normalerweise produktivsten Tagen (kurz vor Abgabe) auch noch unglücklicherweise durch Krankheit zurückgehalten wurde. 
\end{abstract}

\section{Einleitung}
\IEEEPARstart{D}{as} Datenvolumen, welches weltweit produziert wird, steigt jedes Jahr weiter an und damit auch die Herausforderungen hinsichtlich Speicherbedarf, Datenübertragungsraten und Abfrageeffizienz. Im Zentrum dieser Problematik stehen Datenbanksysteme, da diese jetzt und in Zukunft die fundamentale Infrastruktur zahlreicher Anwendungen sind. Diese müssen die immer größer werdenden Datenvolumina dauerhaft verwalten und insbesondere auch performant bereitstellen. Aus diesen Gründen gewinnen Textkompressionsalgorithmen zunehmend an Bedeutung, da sie es ermöglichen textbasierte Daten komprimiert zu speichern und im Optimalfall gleichzeitig auch Abfragen effizienter durchzuführen, ohne die Daten vollständig dekomprimieren zu müssen. Dadurch leisten diese einen enormen Beitrag zur Optimierung der Speichernutzung als auch der Verarbeitungsgeschwindigkeit und helfen bei der Bewältigung der momentanen digitalen Transformation auf der Welt.

Die Forschungshistorie im Bereich der Textkompressionsalgorithmen geht sehr weit in die Vergangenheit, aber durch neuere Technologien und Erkenntnisse in diesem Bereich entstehen immer wieder neue Anforderungen und Herausforderungen. Es existieren viele verschiedene Arten von Datenbanken, wobei jede andere Vor- und Nachteile mit sich bringt. Ziel dieser Arbeit ist es einen Überblick über die aktuelle Forschung im Bereich von Textkompressionsalgorithmen in Datenbanken zu erarbeiten. Um dieses Ziel zu erreichen sollen zuerst die Grundlagen von Textkomprimierung und Datenbanken geschaffen werden. Außerdem sollen verschieden Forschungsansätze betrachtet werden und mit gängigen Verfahren verglichen werden.

Aufbau der Arbeit --TODO--

\section{Hauptteil}
\subsection{Verwandte Arbeiten}
Raichand und Aggarwal \cite{Raichand} stellen einen grundlegenden Überblick über die bestehende Forschung zu Kompressionsverfahren in spaltenorientierten Datenbanksystemen bereit. Das Paper betrachtet die historische Entwicklung in diesem Bereich genauer und zeigt, dass Kompressionstechniken schon lange bekannt waren, bevor sie im Kontext der Datenbankperformanz unterscuht wurden. Es werden verschiedene zentrale algorithmische Ansätze, wie \textbf{Wörterbuchkodierung}, \textbf{Lauflängenkodierung}, \textbf{Nullunterdrückung} und \textbf{Lempel-Ziv-basierte} Verfahren zusammengefasst. Außerdem hebt der Artikel hervor warum spaltenorientierte Datenbanksysteme aufgrund ihrer Speicherorganisation besonders hohe Kompressionsraten erzielen. -TODO- Vergleich mit/Abgrenzung zur eigenen Arbeit

-TODO- weitere verwandte Arbeiten

\subsection{Konzept}
Diese Sektion führt die grundlegenden Konzepte und Begriffe ein, die für das Verständnis von Textkompressionsverfahren im Kontext von Datenbanksystemen erforderlich sind. Ziel ist es, einen einheitlichen begrifflichen und theoretischen Rahmen zu schaffen, der die Einordnung und Bewertung der in den folgenden Kapiteln diskutierten Algorithmen und Forschungsarbeiten ermöglicht.

\subsubsection{\textbf{Verlustfreie und verlustbehaftete Kompression}}
Datenkompression bezeichnet Verfahren zur Verringerung des Speicherbedarfs digitaler Informationen. Diese können grundsätzlich in zwei Klassen eingeteilt werden: \textbf{verlustfreie} und \textbf{verlustbehaftete Kompression}.

Wie der Name es schon vermuten lässt, können bei der \textbf{verlustfreien Kompression} die ursprünglichen Daten vollständig und bitgenau rekonstruiert werden. Verlustfreie Verfahren nutzen daher die inhärente Redundanz in Daten, indem sie häufig vorkommende Muster effizient kodieren.\cite{salomon}

Demgegenüber stehen \textbf{verlustbehaftete Verfahren}, bei denen die Rekonstruktion nur eine approximierte Version der ursprünglichen Daten liefert. Dadurch können für den Menschen oder die Anwendung weniger relevante Informationen gezielt entfernt werden und deutlich höhere Kompressionsraten erreicht werden.\cite{salomon}

Im Kontext von Datenbanksystemen ist vor allem die verlustfreie Kompression von Bedeutung, da die gespeicherten Daten semantisch exakt erhalten bleiben sollen. Dennoch gibt es spezialisierte Szenarien in welchen auch die verlustbehaftete Kompression Anwendung findet.

\subsubsection{\textbf{Prinzipien der Informationsreduktion}}
Drei zentrale theoretischen Konzepte, die die Wirksamkeit und Grenzen von Kompressionsverfahren bestimmen, sind \textbf{Entropie}, \textbf{Redundanz} und \textbf{Kodierung}. Diese Prinzipien bilden die Basis nahezu aller modernen Kompressionsalgorithmen und sind der Schlüssel dafür, weshalb bestimmte Verfahren für bestimmte Datentypen besonders gut geeignet sind.

Die \textbf{Entropie} beschreibt die durchschnittliche Informationsmenge, die in einem Symbol einer Datenquelle enthalten ist. Somit definiert sie eine theoretisch Untergrenze für das maximale Kompressionspotenzial einer Datenquelle. Enthalten die zu komprimierenden Daten wenig strukturelle Muster, bedeutet das, dass die Quelle hohe Entropie hat und die Daten damit nur begrenzt komprimierbar sind. Daraus folgt, dass Daten mit geringer Entropie prinzipiell hohe Kompressionsraten ermöglichen. \cite{salomon}

\textbf{Redundanz} bezeichnet den Anteil von Information in einer Datenquelle, der keine neue Information enthält. Dieser Anteil kann daher entfernt werden, ohne die Fähigkeit zur exakten Rekonstruktion der ursprünglichen Daten zu verlieren. Im Bereich der Datenkompression entsteht Redundanz oft durch statistische Regelmäßigkeiten, wiederkehrende Muster oder strukturelle Korrelationen innerhalb der Daten. Dieses Prinzip bildet die grundlegende Voraussetzung für die Reduktion des Datenumfangs. \cite{salomon}

\textbf{Kodierung} beschreibt im Kontext der Datenkompression die systematische Zuordnung von Symbolen einer Datenquelle zu Codewörtern. Die Länge und Struktur der Codewörter wird dabei an die statistischen Eigenschaften der Quelle angepasst. Durch diese Abbildung können häufig auftretende Symbole effizienter repräsentiert werden. Das sorgt insgesamt für eine kompaktere Gesamtdarstellung der Daten. Damit die ursprünglichen Daten verlustfrei rekonstruiert werden können, muss die Zuordnung eindeutig dekodierbar bleiben. \cite{salomon}

\subsubsection{\textbf{Modellierungsansätze der Kompression}}
Kompressionsverfahren variieren nicht nur in Bezug auf ihre spezifischen Algorithmen, sondern auch hinsichtlich der Methodik, mit der sie die zugrunde liegende Datenquelle modellieren. Der gewählte Modellierungsansatz hat maßgeblichen Einfluss darauf, welche Arten von Redundanz genutzt werden können und wie effizient eine Kompression ist. Die Literatur differenziert zwischen zwei grundlegenden Ansätzen: \textbf{statistische Modellierung} und \textbf{wörterbuchbasierte Modellierung}. \cite{salomon}

Bei der \textbf{statistischen Modellierung} wird die Datenquelle durch Wahrscheinlichkeitsverteilungen ihrer Symbole beschrieben. Ziel ist es, die Häufigkeit des Auftretens einzelner Symbole oder Symbolfolgen zu erfassen und diese Information zur Konstruktion effizienter Codes zu nutzen. Symbole mit hoher Auftretenswahrscheinlichkeit werden dabei kürzer kodiert als seltene Symbole. Die theoretische Grundlage dieses Ansatzes bildet die Informationstheorie, insbesondere das Konzept der Entropie, welche eine untere Schranke für die durchschnittliche Codewortlänge definiert. Zu den bekanntesten Verfahren dieses Ansatzes zählen variable-Längen-Kodierungen, Präfixcodes sowie Huffman- und arithmetische Kodierung. Statistische Verfahren eignen sich besonders für Datenquellen mit stabilen oder gut approximierbaren Wahrscheinlichkeitsverteilungen. \cite{salomon}


Demgegenüber steht die \textbf{wörterbuchbasierte Modellierung}, bei der wiederkehrende Muster, Teilstrings oder Symbolsequenzen explizit identifiziert und durch Referenzen auf ein dynamisch oder statisch aufgebautes Wörterbuch ersetzt werden. Anstatt einzelne Symbole probabilistisch zu bewerten, konzentriert sich dieser Ansatz auf strukturelle Wiederholungen innerhalb der Daten. Die bekanntesten Vertreter dieses Modells sind die Lempel-Ziv-Verfahren, darunter \textbf{LZ77}, \textbf{LZ78} und deren Weiterentwicklungen. Wörterbuchbasierte Verfahren sind insbesondere bei textuellen Daten und Daten mit hoher Wiederholungsrate effektiv, da sie ohne explizite Wahrscheinlichkeitsmodelle auskommen und sich adaptiv an die Struktur der Eingabedaten anpassen können. \cite{salomon}


In der Praxis werden beide Modellierungsansätze häufig kombiniert, um ihre jeweiligen Stärken auszunutzen. Ein prominentes Beispiel hierfür ist das Deflate-Verfahren, das eine Lempel-Ziv-basierte Vorverarbeitung mit anschließender statistischer Kodierung verbindet. Solche hybriden Ansätze sind auch im Kontext von Datenbanksystemen weit verbreitet, da sie sowohl gute Kompressionsraten als auch akzeptable Laufzeiten ermöglichen. Die Wahl des Modellierungsansatzes stellt somit einen zentralen Designentscheid bei der Entwicklung und Auswahl von Kompressionsverfahren dar. \cite{salomon}
\subsubsection{\textbf{Bewertungskriterien von Kompressionsverfahren}}
Zur Beurteilung und zum Vergleich von Kompressionsverfahren werden verschiedene quantitative und qualitative Kriterien herangezogen. Diese Bewertungskriterien erfassen sowohl die Effizienz der Datenreduktion als auch die Auswirkungen auf Rechenaufwand, Speicherverbrauch und Anwendbarkeit. Eine isolierte Betrachtung einzelner Kennzahlen ist dabei meist nicht ausreichend, da Kompressionsverfahren typischerweise einen Zielkonflikt zwischen \textbf{Kompressionsrate} und \textbf{Laufzeitverhalten} aufweisen. \cite{salomon}

Ein zentrales Maß ist die \textbf{Kompressionsrate}, die das Verhältnis zwischen der Größe der komprimierten Daten und der ursprünglichen Datenmenge beschreibt. Sie gibt an, welcher Anteil der Originalgröße nach der Kompression verbleibt. Eng damit verwandt ist der \textbf{Kompressionsfaktor}, der das inverse Verhältnis darstellt und angibt, um welchen Faktor die Datenmenge reduziert wurde. Diese Kennzahlen erlauben eine direkte Aussage über die erzielte Platzersparnis, sagen jedoch nichts über den erforderlichen Rechenaufwand aus. \cite{salomon}

Neben der reinen Datenreduktion spielt die \textbf{Geschwindigkeit der Kompression und Dekompression} eine wesentliche Rolle. Sie wird häufig in Form des Durchsatzes oder der benötigten Rechenzyklen pro Byte gemessen. Insbesondere in datenbanknahen Anwendungsszenarien ist eine schnelle Dekompression von hoher Bedeutung, da komprimierte Daten oft während der Anfrageverarbeitung gelesen und verarbeitet werden müssen. Verfahren mit hoher Kompressionsrate, aber hohem Rechenaufwand können in solchen Fällen ungeeignet sein. \cite{salomon}

Ein weiteres wichtiges Kriterium ist der \textbf{zusätzliche Speicherbedarf}, den ein Kompressionsverfahren verursacht. Dieser umfasst etwa Tabellen, Wörterbücher oder Hilfsstrukturen, die für Kodierung und Dekodierung erforderlich sind. Verfahren mit großem Speichermehraufwand können insbesondere bei großen Datenmengen oder in speicherbeschränkten Systemen nachteilig sein. Ebenso relevant ist die Frage, ob ein Verfahren im sequentiell oder blockweise arbeitet, da dies Einfluss auf Latenz und Speicherbedarf hat. \cite{salomon}

Für den Einsatz in Datenbanksystemen gewinnt zudem die Unterstützung von \textbf{direktem Zugriff} an Bedeutung. Kompressionsverfahren, die einen direkten Zugriff auf einzelne Datenwerte oder Datenbereiche ermöglichen, ohne eine vollständige Dekompression durchzuführen, sind hier besonders vorteilhaft. Diese Eigenschaft beeinflusst maßgeblich die Eignung eines Verfahrens für unterschiedliche Datenbankarchitekturen und Abfragearten. \cite{salomon}

Insgesamt zeigt sich, dass die Bewertung von Kompressionsverfahren stets anwendungsabhängig erfolgen muss. Während einige Verfahren auf maximale Datenreduktion optimiert sind, priorisieren andere geringe Latenz oder einfache Implementierbarkeit. Die Wahl eines geeigneten Verfahrens stellt daher einen Kompromiss zwischen mehreren, teils konkurrierenden Bewertungskriterien dar. \cite{salomon}
\subsubsection{\textbf{Gängige Kompressionsalgorithmen}}github 
Aufbauend auf den beschriebenen Prinzipien und Modellierungsansätzen haben sich im Laufe der Zeit verschiedene Kompressionsalgorithmen etabliert. Je nach Datentyp und Anwendungsszenario weisen diese unterschiedliche Eigenschaften auf.  \cite{salomon}

Die \textbf{Huffman-Kodierung} ist eines der bekanntesten statistischen Kompressionsverfahren. Sie basiert auf der Konstruktion eines präfixfreien Codes, bei dem die Länge der Codewörter invers proportional zur Auftretenswahrscheinlichkeit der jeweiligen Symbole ist. Häufig vorkommende Symbole erhalten kurze Codewörter, während seltene Symbole längere Repräsentationen verwenden. Die Huffman-Kodierung ist optimal im Sinne der minimalen mittleren Codewortlänge für ganzzahlige Codewortlängen und wird häufig als Basiskomponente in komplexeren Kompressionsverfahren eingesetzt. \cite{salomon}

\textbf{Lempel-Ziv-basierte Kompressionsverfahren} verfolgen einen wörterbuchbasierten Ansatz und identifizieren wiederkehrende Muster oder Teilstrings innerhalb der Eingabedaten. Anstatt diese mehrfach zu speichern, werden Referenzen auf bereits bekannte Sequenzen verwendet. Zu den klassischen Vertretern zählen LZ77, LZ78 und LZW, die sich insbesondere in der Art und Weise der Wörterbuchverwaltung unterscheiden. Diese Verfahren arbeiten adaptiv und benötigen keine vorgelagerte statistische Analyse der Daten, was sie besonders flexibel für unterschiedliche Datentypen macht. Aufgrund ihrer Effizienz und Robustheit bilden Lempel-Ziv-Verfahren die Grundlage zahlreicher verbreiteter Formate und Systeme. \cite{salomon}

Die \textbf{Lauflängenkodierung} ist ein einfaches strukturelles Kompressionsverfahren, das auf der Komprimierung aufeinanderfolgender identischer Symbole basiert. Wiederholungen werden durch die Kombination aus Symbol und Wiederholungsanzahl ersetzt. Lauflängenkodierung ist besonders effektiv bei Daten mit langen Sequenzen identischer Werte, etwa bei bestimmten Textmustern oder spaltenorientierten Daten mit geringer Wertediversität. Bei stark variierenden Daten kann das Verfahren jedoch zu keiner oder sogar negativer Kompression führen. \cite{salomon}

Die \textbf{Wörterbuchkodierung} ersetzt häufig auftretende Werte durch kompakte Referenzen auf ein zentrales Wörterbuch. Dieses Verfahren wird insbesondere in spaltenorientierten Datenbanksystemen eingesetzt, da dort oft viele identische Attributwerte auftreten. Ergänzend dazu reduziert die Nullunterdrückung den Speicherbedarf, indem explizite Repräsentationen von Null- oder Standardwerten vermieden werden. Beide Verfahren zeichnen sich durch geringen Rechenaufwand und die Möglichkeit des direkten Zugriffs auf einzelne Datenwerte aus, was sie für datenbanknahe Anwendungen besonders geeignet macht. \cite{salomon}

Neben den klassischen Verfahren haben sich moderne \textbf{Hochgeschwindigkeitscodecs} wie \textbf{Snappy} oder \textbf{LZ4} etabliert. Diese Algorithmen priorisieren geringe Latenz und hohe Dekompressionsgeschwindigkeit gegenüber maximaler Kompressionsrate. Sie basieren häufig auf vereinfachten Lempel-Ziv-Varianten und sind für den Einsatz in speicher- und abfrageintensiven Systemen konzipiert. In Datenbanksystemen werden sie bevorzugt eingesetzt, wenn schnelle Anfrageverarbeitung wichtiger ist als eine maximale Reduktion des Speicherbedarfs. \cite{salomon}
\subsubsection{\textbf{Datenbankarchitekturen und ihre Anforderungen}}
Datenbanksysteme lassen sich hinsichtlich ihres Datenmodells und ihrer physischen Speicherorganisation in unterschiedliche Architekturen einteilen. Die gewählte Architektur beeinflusst maßgeblich, wie Daten gespeichert, abgerufen und verarbeitet werden und bestimmt damit auch die Anforderungen an geeignete Kompressionsverfahren. Eine grundlegende Unterscheidung erfolgt zwischen \textbf{relationalen Datenbanksystemen} und nicht-relationalen, sogenannten \textbf{NoSQL-Datenbanken}, die jeweils unterschiedliche Designziele verfolgen. \cite{kaufmann}

\textbf{Relationale Datenbanksysteme} basieren auf einem tabellarischen Datenmodell mit festem Schema und speichern Daten typischerweise zeilenorientiert. Dabei werden alle Attribute eines Tupels physisch gemeinsam abgelegt, was effiziente Zugriffe auf vollständige Datensätze begünstigt. Diese Architektur ist insbesondere für transaktionsorientierte Anwendungen ausgelegt, bei denen Konsistenz, Integrität und häufige Schreiboperationen im Vordergrund stehen. Für Kompressionsverfahren ergibt sich daraus die Anforderung, dass einzelne Tupel mit geringer Latenz dekomprimiert werden können. Verfahren mit hohem Speichermehraufwand oder ausschließlich blockweiser Verarbeitung sind in diesem Kontext weniger geeignet, da sie den direkten Zugriff auf einzelne Datensätze erschweren. \cite{kaufmann}

\textbf{Spaltenorientierte Datenbanken} organisieren ihre Daten nicht nach Tupeln, sondern nach Attributen, sodass alle Werte einer Spalte zusammenhängend gespeichert werden. Diese Architektur wird insbesondere in analytischen Szenarien eingesetzt, in denen Abfragen häufig nur auf wenige Attribute zugreifen. Durch die Homogenität der gespeicherten Werte innerhalb einer Spalte entstehen günstige Voraussetzungen für eine effektive Kompression. Kaufmann und Meier heben hervor, dass spaltenbasierte Systeme besonders gut für datenreduzierende Techniken geeignet sind, da Redundanzen innerhalb einzelner Attribute effizient ausgenutzt werden können. Daraus ergeben sich Anforderungen an Kompressionsverfahren, die eine partielle Dekompression sowie direkten Zugriff auf einzelne Werte oder Wertebereiche unterstützen. \cite{kaufmann}

\textbf{NoSQL-Datenbanken} verfolgen alternative Datenmodelle, um Skalierbarkeit, Verfügbarkeit und Flexibilität zu erhöhen. Zu den wichtigsten Kategorien zählen Schlüssel-Wert-Speicher, dokumentorientierte Datenbanken, spaltenbasierte NoSQL-Systeme sowie Graphdatenbanken. Diese Systeme verzichten häufig auf ein starres Schema und sind für den Betrieb in verteilten Umgebungen konzipiert. Aufgrund der heterogenen Datenstrukturen und dynamischen Zugriffsmuster kommen hier meist generische, robuste Kompressionsverfahren zum Einsatz, die im Streaming-Betrieb arbeiten und nur geringen zusätzlichen Speicherbedarf verursachen. Die maximale Kompressionsrate tritt dabei häufig hinter Durchsatz und geringer Latenz zurück. \cite{kaufmann}

Zusammenfassend lassen sich mehrere architekturabhängige Anforderungen an Kompressionsverfahren identifizieren. Zeilenorientierte Systeme erfordern eine schnelle, tupelweise Dekompression, während spaltenorientierte Datenbanken Verfahren begünstigen, die hohe Kompressionsraten bei gleichzeitigem direktem Zugriff ermöglichen. NoSQL-Systeme stellen hingegen Anforderungen an Skalierbarkeit, geringe Metadatenkosten und effiziente sequentielle Verarbeitung. Die Auswahl eines geeigneten Kompressionsverfahrens ist daher eng an die jeweilige Datenbankarchitektur sowie den dominierenden Workload gekoppelt. \cite{kaufmann}

\subsubsection{\textbf{Konzeptioneller Rahmen der Arbeit}}
Die in diesem Kapitel vorgestellten Grundlagen bilden den konzeptionellen Rahmen für die weitere Untersuchung von Textkompressionsalgorithmen in Datenbanksystemen. Durch die Einführung zentraler Begriffe, Modellierungsansätze und Bewertungskriterien wird eine einheitliche Basis geschaffen, die eine strukturierte Analyse bestehender Forschungsarbeiten ermöglicht. Insbesondere die Unterscheidung zwischen statistischen und wörterbuchbasierten Kompressionsansätzen sowie die Betrachtung architekturabhängiger Anforderungen dient als Grundlage für die systematische Einordnung der im weiteren Verlauf diskutierten Verfahren.

Darüber hinaus erlauben die definierten Bewertungskriterien, wie Kompressionsrate, Laufzeitverhalten, Speichermehraufwand und Unterstützung von direktem Zugriff, einen vergleichbaren Blick auf unterschiedliche Ansätze, unabhängig von ihrer konkreten Implementierung. Der folgende Hauptteil nutzt diesen konzeptionellen Rahmen, um relevante Forschungsarbeiten einzuordnen, gegenüberzustellen und deren Eignung für verschiedene Datenbankarchitekturen zu diskutieren.

\subsection{Implementierung}
Dieses Kapitel setzt den im vorherigen Abschnitt entwickelten konzeptionellen Rahmen in die Praxis um. Da es sich bei dieser Arbeit um eine Forschungsfeldübersicht handelt, besteht die Umsetzung nicht in der Implementierung eines konkreten Kompressionsverfahrens, sondern in der systematischen Auswahl, Analyse und Gegenüberstellung relevanter wissenschaftlicher Arbeiten zu Textkompressionsalgorithmen im Datenbankkontext. Ziel ist es, bestehende Ansätze anhand der zuvor definierten Kriterien einzuordnen und vergleichbar zu machen.

Die Identifikation geeigneter Arbeiten erfolgte durch eine gezielte Literaturrecherche in etablierten wissenschaftlichen Datenbanken und Konferenzarchiven, insbesondere arXiv, ACM Digital Library und Proceedings of the VLDB Endowment. Dabei wurden Suchbegriffe wie string compression, text compression, dictionary compression sowie random access compression in Kombination mit database systems verwendet. Ergänzend wurden Referenzen aus einschlägigen Survey- und Systempapern berücksichtigt, um relevante Arbeiten nicht isoliert, sondern im Kontext des bestehenden Forschungsstands zu erfassen.

Aus der so gewonnenen Menge an Publikationen wurden diejenigen Arbeiten ausgewählt, die einen klaren Bezug zu Text- bzw. Stringkompression in Datenbanksystemen aufweisen und explizit architekturspezifische Anforderungen adressieren. Auswahlkriterien waren unter anderem der Fokus auf verlustfreie Verfahren, die Unterstützung von direktem Zugriff auf komprimierte Daten, experimentelle Evaluationen im Datenbankkontext sowie die Relevanz für spaltenorientierte oder In-Memory-Systeme. Dadurch wird sichergestellt, dass die betrachteten Arbeiten sowohl konzeptionell als auch praktisch vergleichbar sind.

Der Vergleich der ausgewählten Arbeiten erfolgt entlang der im Konzeptteil eingeführten Dimensionen. Dazu zählen insbesondere der zugrunde liegende Modellierungsansatz, die erzielte Kompressionsrate, das Laufzeitverhalten bei Kompression und Dekompression, der Speichermehraufwand sowie die Unterstützung von direktem Zugriff und effizienter Anfrageverarbeitung. Die folgenden Unterabschnitte stellen die ausgewählten Arbeiten jeweils einzeln vor und ordnen sie anschließend vergleichend ein, um Gemeinsamkeiten, Unterschiede und bestehende Zielkonflikte herauszuarbeiten.

\subsubsection{\textbf{FSST: Fast Random Access String Compression}}
Boncz et al. stellen mit \textbf{FSST (Fast Static Symbol Table)} \cite{fsst} ein verlustfreies Kompressionsverfahren für Zeichenketten vor, das speziell für den Einsatz in modernen In-Memory- und spaltenorientierten Datenbanksystemen konzipiert ist. Ziel des Ansatzes ist es, eine hohe Dekompressionsgeschwindigkeit mit Unterstützung von direktem Zugriff auf einzelne Zeichenketten zu kombinieren, ohne dabei aufwendige dynamische Wörterbuchstrukturen zu verwenden.

Der Ansatz von FSST basiert auf einer statischen Wörterbuchkodierung, bei der häufig vorkommende Byte-Sequenzen durch kurze Codes ersetzt werden. Im Gegensatz zu klassischen Lempel-Ziv-Verfahren erfolgt die Wörterbucherstellung einmalig in einer Trainingsphase und bleibt während der eigentlichen Kompression unverändert. Dadurch wird ein deterministisches und sehr effizientes Dekodierverfahren ermöglicht, das sich besonders für datenbanknahe Workloads eignet.

Ein zentrales Merkmal von FSST ist die explizite Optimierung auf Random-Access-Zugriffe. Jede komprimierte Zeichenkette kann unabhängig von anderen Strings dekomprimiert werden, ohne dass vorherige Daten verarbeitet werden müssen. Diese Eigenschaft ist insbesondere für Datenbanksysteme relevant, da Abfragen häufig selektiv auf einzelne Tupel oder Attributwerte zugreifen. Der notwendige Speichermehraufwand für das statische Wörterbuch ist begrenzt und amortisiert sich bei größeren Datenmengen durch die erzielte Speicherersparnis.

Die experimentelle Evaluation zeigt, dass FSST im Vergleich zu etablierten Verfahren wie Snappy oder LZ4 eine deutlich höhere Dekompressionsgeschwindigkeit erreicht, während die Kompressionsrate in einem ähnlichen Bereich liegt. Besonders in analytischen Szenarien mit stringlastigen Attributen bietet FSST damit einen günstigen Kompromiss zwischen Speicherreduktion und Laufzeitverhalten. Aufgrund dieser Eigenschaften eignet sich der Ansatz insbesondere für spaltenorientierte und In-Memory-Datenbanksysteme.

Aufbauend auf dem FSST-Verfahren stellen die Autoren von GSST (GPU Static Symbol Table) \cite{gsst} eine architekturspezifische Erweiterung vor, die die Dekompression von FSST-kodierten Zeichenketten für den Einsatz auf GPUs optimiert. Der Ansatz verfolgt dabei nicht das Ziel, die zugrunde liegende Kompressionsmethode zu verändern, sondern konzentriert sich auf eine effiziente Parallelisierung der Dekompression unter Beibehaltung des FSST-Datenformats.

GSST nutzt die statische Symboltabelle und die deterministische Dekodierbarkeit von FSST, um komprimierte Strings in unabhängige Teilbereiche zu zerlegen, die parallel verarbeitet werden können. Hierzu werden zusätzliche Metadaten eingeführt, welche eine koordinierte Dekompression durch viele Threads ermöglichen. Die vorgeschlagenen Anpassungen sind eng an die Eigenschaften von FSST gekoppelt und lassen sich nicht ohne Weiteres auf andere Kompressionsverfahren übertragen.

Die Evaluation zeigt, dass GSST auf moderner GPU-Hardware eine sehr hohe Dekompressionsbandbreite erreicht und damit insbesondere für analytische Workloads mit stringlastigen Attributen geeignet ist. Gleichzeitig verdeutlicht der Ansatz, dass die Wahl eines geeigneten Kompressionsformats eine entscheidende Voraussetzung für architekturspezifische Optimierungen darstellt. GSST ist somit nicht als eigenständiges Kompressionsverfahren zu verstehen, sondern als spezialisierte Erweiterung von FSST, die dessen Einsatzbereich auf stark parallelisierte Hardwareplattformen ausdehnt.

\subsubsection{\textbf{Dictionary-based Order-preserving String Compression for Main Memory Column Stores}}
Binnig et al. \cite{binnig} stellen ein wörterbuchbasiertes Kompressionsverfahren vor, das speziell für den Einsatz in Datenbanksystemen entwickelt wurde, bei denen Ordnungseigenschaften von Zeichenketten erhalten bleiben müssen. Der zentrale Beitrag der Arbeit besteht darin, Kompression mit der Anforderung zu kombinieren, dass die lexikographische Ordnung der Originalstrings auch in der komprimierten Darstellung erhalten bleibt. Diese Eigenschaft ist insbesondere für Datenbanksysteme relevant, da viele Operationen wie Sortierungen, Bereichsanfragen oder Indexzugriffe auf der natürlichen Ordnung von Strings basieren.

Der vorgestellte Ansatz verwendet eine Wörterbuchkodierung, bei der häufig auftretende Zeichenketten oder Teilstrings durch kompakte Codes ersetzt werden. Im Unterschied zu klassischen Dictionary-Encoding-Verfahren wird das Wörterbuch jedoch so konstruiert, dass die resultierenden Codes die ursprüngliche lexikographische Ordnung der Werte widerspiegeln. Dadurch können Vergleichsoperationen direkt auf den komprimierten Repräsentationen durchgeführt werden, ohne eine vorherige Dekompression der Daten zu erfordern.

Ein wesentlicher Vorteil dieses Ansatzes liegt in der Unterstützung von direktem Zugriff und vergleichsbasierter Anfrageverarbeitung auf komprimierten Daten. Datenbankoperatoren wie Selektionen oder Sortierungen können effizient ausgeführt werden, da die Ordnungseigenschaften der Daten erhalten bleiben. Gleichzeitig bleibt der Speichermehraufwand für das Wörterbuch überschaubar, da es auf die im Datenbestand tatsächlich vorkommenden Strings beschränkt ist.

Die experimentellen Ergebnisse zeigen, dass das vorgestellte Verfahren insbesondere für spaltenorientierte Datenbanksysteme geeignet ist, in denen Zeichenkettenattribute häufig sortiert oder gruppiert verarbeitet werden. Im Vergleich zu nicht ordnungserhaltenden Kompressionsverfahren werden zwar teilweise geringere Kompressionsraten erzielt, dieser Nachteil wird jedoch durch die Einsparung von Dekompressionskosten während der Anfrageverarbeitung kompensiert. Der Ansatz verdeutlicht damit einen wichtigen Zielkonflikt zwischen maximaler Datenreduktion und effizienter Ausführung datenbanktypischer Operationen.

\subsubsection{\textbf{Fast \& Strong: The Case of Compressed String Dictionaries on Modern CPUs}}
In diesem Paper \cite{lasch} untersuchen Lasch et al. den Zielkonflikt zwischen hoher Kompressionsrate und effizientem Laufzeitverhalten in spaltenorientierten Datenbanksystemen. Die Arbeit adressiert die Frage, inwieweit stärkere Kompressionsverfahren, die üblicherweise mit höherem Rechenaufwand verbunden sind, dennoch für analytische Datenbankworkloads geeignet sein können. Der Fokus liegt dabei auf der systematischen Kombination und Bewertung mehrerer Kompressionstechniken innerhalb eines spaltenorientierten Speicherlayouts.

Der vorgestellte Ansatz nutzt eine mehrstufige Kompressionspipeline, bei der einfache Verfahren wie Wörterbuchkodierung oder Lauflängenkodierung mit stärker komprimierenden Techniken kombiniert werden. Ziel ist es, die Redundanzen innerhalb einzelner Spalten möglichst effektiv auszunutzen, ohne die Dekompressionskosten für typische Abfrageoperationen übermäßig zu erhöhen. Die Autoren zeigen, dass sich durch eine geeignete Auswahl und Kombination der Verfahren sowohl hohe Kompressionsraten als auch akzeptable Zugriffslatenzen erzielen lassen.

Ein zentrales Ergebnis der Arbeit ist die Beobachtung, dass stärker komprimierte Daten nicht zwangsläufig zu schlechterer Abfrageperformance führen. Insbesondere bei I/O-lastigen analytischen Workloads kann die reduzierte Datenmenge die zusätzlichen Dekompressionskosten kompensieren oder sogar überwiegen. Damit widerspricht die Arbeit der verbreiteten Annahme, dass schnelle, aber schwach komprimierende Verfahren grundsätzlich vorzuziehen sind.

Die Evaluation erfolgt anhand typischer analytischer Abfragen auf spaltenorientierten Datenbeständen und verdeutlicht, dass die optimale Wahl eines Kompressionsverfahrens stark vom jeweiligen Workload abhängt. Die Ergebnisse unterstreichen die Bedeutung einer architektur- und workloadabhängigen Auswahl von Kompressionsstrategien und liefern wichtige Erkenntnisse für das Design moderner analytischer Datenbanksysteme.

\subsubsection{\textbf{AlphaZip: Neural Network-Enhanced Lossless Text Compression}}
Mit ALPHAZIP \cite{alpha} präsentieren die Autoren einen Ansatz zur verlustfreien Textkompression, der neuronale Netze zur Modellierung der Eingabedaten nutzt. Im Gegensatz zu klassischen statistischen oder wörterbuchbasierten Verfahren zielt die Arbeit darauf ab, komplexe Abhängigkeiten und Muster in Textdaten durch lernbasierte Modelle zu erfassen, um eine genauere Vorhersage der nächsten Symbole zu ermöglichen. Der Ansatz ordnet sich damit in die wachsende Forschung zu maschinellem Lernen für Datenkompression ein.

ALPHAZIP kombiniert ein neuronales Sprachmodell mit einem nachgelagerten Entropiekodierungsverfahren. Das neuronale Netz wird darauf trainiert, Wahrscheinlichkeitsverteilungen für die nächsten Zeichen vorherzusagen, die anschließend für eine effiziente Kodierung genutzt werden. Durch die Verwendung tiefer neuronaler Netze können auch langfristige Abhängigkeiten in Texten berücksichtigt werden, was potenziell zu höheren Kompressionsraten führt als bei klassischen Modellen mit begrenztem Kontext.

Die experimentellen Ergebnisse zeigen, dass ALPHAZIP bei allgemeinen Textkorpora teilweise bessere Kompressionsraten erzielt als etablierte verlustfreie Verfahren. Gleichzeitig wird jedoch deutlich, dass dieser Gewinn mit erheblichem Rechenaufwand verbunden ist. Insbesondere das Training und die Inferenz der neuronalen Modelle erfordern deutlich mehr Rechenzeit und Speicherressourcen als klassische Kompressionsalgorithmen. Zudem ist der Ansatz nicht auf direkten Zugriff oder partielle Dekompression einzelner Strings ausgelegt, da die Dekodierung sequenziell erfolgt.

Im Kontext von Datenbanksystemen ergeben sich daraus deutliche Einschränkungen. Zwar demonstriert ALPHAZIP das Potenzial lernbasierter Modelle für die Textkompression, die fehlende Unterstützung von direktem Zugriff sowie die hohen Laufzeit- und Ressourcenanforderungen machen den Ansatz für datenbanknahe Anwendungen derzeit nur eingeschränkt geeignet. Die Arbeit ist daher weniger als praktikable Lösung für Datenbanksysteme zu verstehen, sondern vielmehr als explorativer Beitrag, der zeigt, wie maschinelles Lernen zur weiteren Verbesserung von Kompressionsraten eingesetzt werden kann.

\subsubsection{\textbf{Vergleich und Einordnung der Ansätze}}
Die im vorherigen Abschnitt vorgestellten Arbeiten verfolgen unterschiedliche Zielsetzungen und adressieren verschiedene Anforderungen an Textkompression im Datenbankkontext. Ein systematischer Vergleich ist daher nur entlang klar definierter Kriterien sinnvoll. Im Folgenden werden die Ansätze anhand des zugrunde liegenden Modellierungsansatzes, der erzielten Kompressionswirkung, des Laufzeitverhaltens, des Speichermehraufwands sowie ihrer Eignung für datenbanktypische Zugriffsmuster eingeordnet.

Hinsichtlich des Modellierungsansatzes lassen sich zwei Hauptgruppen unterscheiden. FSST sowie der order-preserving Ansatz von Binnig et al. basieren auf statischer Wörterbuchkodierung, wobei FSST primär auf schnelle Dekompression und direkten Zugriff optimiert ist, während der Ansatz von Binnig et al. zusätzlich Ordnungseigenschaften erhält. Die Arbeit von Lasch et al. kombiniert mehrere klassische Verfahren innerhalb einer spaltenorientierten Pipeline und verfolgt damit einen stärker aggregierten Ansatz zur Reduktion von Redundanz. ALPHAZIP hingegen unterscheidet sich grundlegend von allen anderen Arbeiten, da ein neuronales Modell zur probabilistischen Vorhersage von Symbolen eingesetzt wird.

Im Hinblick auf die Kompressionsrate zeigt sich, dass stärkere, teilweise mehrstufige Verfahren tendenziell bessere Ergebnisse erzielen. Insbesondere die in \textit{Fast \& Strong} untersuchten Kombinationen erreichen hohe Datenreduktionen, profitieren jedoch von analytischen Workloads mit hohem I/O-Anteil. FSST und der order-preserving Ansatz erzielen im Vergleich dazu moderatere Kompressionsraten, die jedoch bewusst zugunsten anderer Eigenschaften in Kauf genommen werden. ALPHAZIP erreicht auf allgemeinen Textkorpora teilweise sehr gute Kompressionsraten, ist jedoch stark von Trainingsdaten und Modellkomplexität abhängig.

Das Laufzeitverhalten stellt einen zentralen Unterscheidungsfaktor dar. FSST ist explizit auf extrem schnelle Dekompression ausgelegt und eignet sich daher besonders für datenbanknahe Abfrageverarbeitung. Die GPU-basierte Erweiterung GSST verstärkt diese Eigenschaft weiter, indem sie FSST für massiv parallele Hardware optimiert. Der order-preserving Ansatz ermöglicht zwar zusätzliche Einsparungen bei Vergleichsoperationen, verursacht jedoch einen höheren Aufwand bei der Kodierung. Die in Fast and Strong Compression betrachteten Verfahren zeigen, dass höhere Dekompressionskosten in bestimmten Szenarien durch reduzierte I/O-Kosten kompensiert werden können. ALPHAZIP weist im Vergleich dazu deutlich höhere Rechenkosten auf und ist nicht für latenzkritische Anwendungen ausgelegt.

Ein weiterer wesentlicher Aspekt ist der Speichermehraufwand. Wörterbuchbasierte Verfahren wie FSST und der Ansatz von Binnig et al. benötigen zusätzliche Strukturen, deren Größe jedoch begrenzt und kontrollierbar ist. Mehrstufige Kompressionspipelines erhöhen den Speicherbedarf durch mehrere Hilfsstrukturen, was insbesondere bei großen Spalten relevant wird. ALPHAZIP erfordert neben den komprimierten Daten auch die Speicherung neuronaler Modelle, was den Speichermehraufwand deutlich erhöht und den Einsatz in ressourcenbeschränkten Systemen erschwert.

Besonders relevant für Datenbanksysteme ist die Unterstützung von direktem Zugriff und effizienter Anfrageverarbeitung. FSST und dessen Erweiterung GSST ermöglichen den unabhängigen Zugriff auf einzelne Zeichenketten und sind damit gut in bestehende Datenbankoperatoren integrierbar. Der order-preserving Ansatz bietet darüber hinaus den Vorteil, dass Vergleichs- und Sortieroperationen direkt auf komprimierten Daten durchgeführt werden können. Die Verfahren aus \textit{Fast \& Strong} sind primär auf spaltenweise Verarbeitung ausgelegt und eignen sich vor allem für analytische Abfragen. ALPHAZIP hingegen erfordert eine sequentielle Dekompression und ist daher für typische datenbanknahe Zugriffsmuster nur eingeschränkt geeignet.

Zusammenfassend zeigt der Vergleich, dass es kein universell optimales Kompressionsverfahren für alle Datenbankszenarien gibt. Vielmehr stellt die Wahl eines geeigneten Ansatzes einen Kompromiss zwischen Kompressionsrate, Laufzeitverhalten und Zugriffseigenschaften dar. Während FSST und verwandte Erweiterungen besonders für latenzkritische, stringlastige Workloads geeignet sind, adressieren stärkere Kompressionsverfahren vor allem speicher- und I/O-intensive analytische Szenarien. Lernbasierte Ansätze wie ALPHAZIP verdeutlichen das theoretische Potenzial moderner Modelle, zeigen jedoch zugleich die aktuellen Grenzen ihres praktischen Einsatzes im Datenbankkontext auf.

\subsection{Evaluation}

\section{Schluss}

\section{Fragen}
Fachbegriffe immer hervorheben oder nur einmal?
Wo citations?
Reichen 4 Paper + Optimierungspaper?
subsubsections fett?
Abkürzungen für Paper einführen?
Wenn verwendet kursiv oder fett?
Bilder verwenden?
\section{TODOs}
Seitenzahlen bei büchern

\begin{thebibliography}{1}
\bibitem{Raichand}
P. Raichand und R. Aggarwal. ``A SHORT SURVEY OF DATA COMPRESSION TECHNIQUES FOR
COLUMN ORIENTED DATABASES'' in  \textit{Journal of Global Research in Computer Science, vol. 4, no. 7, pp. 43-46, 2013.}

\bibitem{salomon}
D. Salomon. ``Data Compression – The Complete Reference'', \textit{3rd ed., Springer, 2004.}

\bibitem{kaufmann}
M. Kaufmann und A. Meier. ``SQL- \& NoSQL-Datenbanken'', \textit{9th ed., Springer, 2023.}

\bibitem{fsst}
P. Boncz, T. Neumann und V. Leis. ``FSST: Fast random access string compression'', \textit{Proceedings of the VLDB Endowment 13, pp. 2649–2661, 2020.}

\bibitem{binnig}
C. Binnig, S. Hildebrand und F. Färber. ``Dictionary-based order-preserving string compression for main memory column stores'', \textit{Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data, pp. 283-296, 2009}

\bibitem{lasch}
R. Lasch, I. Oukid, R. Dementievand, N. May, S. Demirsoy und K. Sattler. ``Fast \& Strong: The Case of Compressed String Dictionaries on Modern CPUs'', \textit{Proceedings of the 15th International Workshop on Data Management on New Hardware, 2019}

\bibitem{alpha}
S. Narashiman und N. Chandrachoodan. ``AlphaZip: Neural Network-Enhanced Lossless Text Compression'', \textit{arXiv, 2024}

\bibitem{gsst}
R. Vonk, J. Hoozemans und Z. Al-Ars. ``GSST: Parallel string decompression at 191 GB/s on GPU'', \textit{Proceedings of the 5th Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems, pp. 8 - 14, 2025}


\end{thebibliography}

\end{document}


